{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows',500)\n",
    "from collections import defaultdict\n",
    "from itertools import islice, combinations\n",
    "from datetime import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from ctgan import CTGANSynthesizer\n",
    "# # 폰트 적용\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cab\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kcs\\df_syn_generation_m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/kcs/df_syn_generation_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터프레임 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org_train = pd.read_csv('./label_org/encoding_train.csv', encoding='utf-8-sig')\n",
    "y_org_train = pd.read_csv('./label_org/y_train.csv', encoding='utf-8-sig')\n",
    "X_org_test = pd.read_csv('./label_org/encoding_valid.csv', encoding='utf-8-sig')\n",
    "y_org_test = pd.read_csv('./label_org/y_valid.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_syn_train = pd.read_csv('./label_syn/encoding_train.csv', encoding='utf-8-sig')\n",
    "y_syn_train = pd.read_csv('./label_syn/y_train.csv', encoding='utf-8-sig')\n",
    "X_syn_test = pd.read_csv('./label_syn/encoding_test.csv', encoding='utf-8-sig')\n",
    "y_syn_test = pd.read_csv('./label_syn/y_test.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 다중알고리즘 성능비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [LogisticRegression(solver='lbfgs'),\n",
    "            DecisionTreeClassifier(max_depth=4),\n",
    "            RandomForestClassifier(n_estimators=100, max_depth=4),  # You can tune some hyper-parameters.\n",
    "            AdaBoostClassifier(),\n",
    "            lgb.LGBMClassifier(),\n",
    "            cab.CatBoostClassifier(depth=4, verbose=False),\n",
    "            xgb.XGBClassifier(n_estimators=100, max_depth=4,objective= 'binary:logistic', eval_metric='logloss')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 원본데이터 성능비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegress,    0.542,  0.1775\n",
      "DecisionTreeCla,    0.6872,  0.0678\n",
      "RandomForestCla,    0.7018,  1.2396\n",
      "AdaBoostClassif,    0.6914,  1.125\n",
      "LGBMClassifier(,    0.7104,  0.1207\n",
      "<catboost.core.,    0.7002,  3.142\n",
      "XGBClassifier(b,    0.6831,  0.6682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for clf in clf_list:\n",
    "        \n",
    "        # Trick to make a unique identifier\n",
    "        exp_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) \n",
    "\n",
    "        # substance\n",
    "        X_train = X_org_train\n",
    "        y_train = y_org_train\n",
    "        X_test = X_org_test\n",
    "        y_test = y_org_test\n",
    "            \n",
    "            \n",
    "        # Check starting time\n",
    "        start = time.time()\n",
    "        \n",
    "        # Train a classifier\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict frauds in test data\n",
    "        y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        y_preds = []\n",
    "        # Append the predictions of each classifier\n",
    "#         np.append(y_pred,y_preds)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "        # Save the predictions of each classifier in the name of 'exp_id' as .npy files\n",
    "        np.save(f'./results/intermediary/y_pred_{exp_id}', y_pred)\n",
    "        np.save(f'./results/intermediary/y_test_{exp_id}', y_test)\n",
    "        \n",
    "        # Save all the models as sav files\n",
    "        filename = f'./results/intermediary/model_{exp_id}.sav'\n",
    "        pickle.dump(clf, open(filename, 'wb'))\n",
    "    \n",
    "        # The reason to choose AUC: \n",
    "        # https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy\n",
    "        clas_auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "\n",
    "        # Check the ending time\n",
    "        done = time.time()\n",
    "        \n",
    "        # Check the runnin time\n",
    "        elapsed = round(done-start, 4)\n",
    "        \n",
    "#         # Print the results (content)\n",
    "#         print(f'{exp_id},{len(X_train)},{len(X_test)},\\\n",
    "#               {str(clf)[:10]},{clas_auc},{elapsed}')\n",
    "#         print(f'{str(clf)[:10]},  {clas_auc},  {elapsed}')\n",
    "\n",
    "#         # Print the results (content)\n",
    "#         print('Classifier','AUC','Time(sec)')\n",
    "        print(f'{str(clf)[:15]},    {clas_auc},  {elapsed}')\n",
    "#         print(\"Classifier: {0:.4f}, AUC: {1:.4f},Time(sec):{2:.4f}\\n\".format(f'{str(clf)[:10]},{clas_auc}, {elapsed}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 가상데이터 성능비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegress,    0.5254,  0.1805\n",
      "DecisionTreeCla,    0.6081,  0.0668\n",
      "RandomForestCla,    0.6351,  1.1342\n",
      "AdaBoostClassif,    0.6186,  1.058\n",
      "LGBMClassifier(,    0.7426,  0.1077\n",
      "<catboost.core.,    0.6912,  3.1281\n",
      "XGBClassifier(b,    0.7033,  0.5136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for clf in clf_list:\n",
    "        \n",
    "        # Trick to make a unique identifier\n",
    "        exp_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) \n",
    "\n",
    "        # substance\n",
    "        X_train = X_syn_train\n",
    "        y_train = y_syn_train\n",
    "        X_test = X_syn_test\n",
    "        y_test = y_syn_test\n",
    "            \n",
    "            \n",
    "        # Check starting time\n",
    "        start = time.time()\n",
    "        \n",
    "        # Train a classifier\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict frauds in test data\n",
    "        y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        y_preds = []\n",
    "        # Append the predictions of each classifier\n",
    "#         np.append(y_pred,y_preds)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "        # Save the predictions of each classifier in the name of 'exp_id' as .npy files\n",
    "        np.save(f'./results/intermediary1/y_pred_{exp_id}', y_pred)\n",
    "        np.save(f'./results/intermediary1/y_test_{exp_id}', y_test)\n",
    "        \n",
    "        # Save all the models as sav files\n",
    "        filename = f'./results/intermediary1/model_{exp_id}.sav'\n",
    "        pickle.dump(clf, open(filename, 'wb'))\n",
    "    \n",
    "        # The reason to choose AUC: \n",
    "        # https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy\n",
    "        clas_auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "\n",
    "        # Check the ending time\n",
    "        done = time.time()\n",
    "        \n",
    "        # Check the runnin time\n",
    "        elapsed = round(done-start, 4)\n",
    "        \n",
    "#         # Print the results (content)\n",
    "#         print(f'{exp_id},{len(X_train)},{len(X_test)},\\\n",
    "#               {str(clf)[:10]},{clas_auc},{elapsed}')\n",
    "#         print(f'{str(clf)[:10]},  {clas_auc},  {elapsed}')\n",
    "\n",
    "#         # Print the results (content)\n",
    "#         print('Classifier','AUC','Time(sec)')\n",
    "        print(f'{str(clf)[:15]},    {clas_auc},  {elapsed}')\n",
    "#         print(\"Classifier: {0:.4f}, AUC: {1:.4f},Time(sec):{2:.4f}\\n\".format(f'{str(clf)[:10]},{clas_auc}, {elapsed}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
